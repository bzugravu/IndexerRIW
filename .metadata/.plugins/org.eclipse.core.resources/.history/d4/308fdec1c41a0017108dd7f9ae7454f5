package HtmlParserApp;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;

public class Frequency {
	
	public List<MapHelper> fileList = new ArrayList<MapHelper>();
	Map<String, List<MapHelper>> map = new HashMap<String, List<MapHelper>>();
	Map<String, Double> idfMap = new HashMap<String, Double>();
	Map<String, Double[]> tfMap = new HashMap<String, Double[]>();
	Map<String, Double> distance = new HashMap<String, Double>();
	public String[] searchQuery;
	
	Double[] query;
	
	public Frequency(SearchHelper sh, Helper h){
		sh.BooleanSearch(h);
		
		System.out.println("***********************************************");
		for(int i=0;i<sh.resultList.size(); i++){
			System.out.println(sh.resultList.get(i).file);
		}			
		System.out.println("***********************************************");
		
		
		fileList = sh.resultList;
		searchQuery = sh.userInput;
		InitializeMapper();
	}
	
	public void InitializeMapper(){
		ObjectMapper mapper = new ObjectMapper();
		try{
			map = mapper.readValue(new File("invertedIndex.json"), new TypeReference<HashMap<String, List<MapHelper>>>() {});
		}catch(Exception ex){
			ex.printStackTrace();
		}
	}
	
	public void TermFrequency(){
		int nrWords = searchQuery.length;
		
		
		for(MapHelper file : fileList){
			Double[] wordTf = new Double[nrWords];
			int index = 0;
			//Add threads
			Map<String, Integer> indexMap = new HashMap<String, Integer>();
			ObjectMapper mapper = new ObjectMapper();	
			
			try{
				indexMap = mapper.readValue(new File("directIndex/" + file.file), new TypeReference<HashMap<String, Integer>>() {});
			}
			catch(IOException e){
				e.printStackTrace();
			}
			//int dim = indexMap.size();
			//Double[] wordTf = new Double[dim+nrWords];
			
			int wordCount = 0;
			for(Integer value : indexMap.values()){
				wordCount += value;
			}
			
			for(String word : searchQuery){
				int wordAppearance;
				if(indexMap.containsKey(word))
					wordAppearance = indexMap.get(word);
				else
					wordAppearance = 0;
				double tempTf = (double)wordAppearance / wordCount;
				double tempIdf = idfMap.get(word);
				wordTf[index] = tempTf * tempIdf;
				index++;
			}
			
//			int wordAppearance;
//			List<String> searchQueryList = Arrays.asList(searchQuery);
//			for(Map.Entry<String, Integer> entry : indexMap.entrySet()){
//				if(searchQueryList.contains(entry.getKey()))
//					continue;
//				wordAppearance = entry.getValue();
//				double tempTf = (double)wordAppearance / wordCount;
//			}
			
			tfMap.put(file.file, wordTf);
		}
		
	}
	
	public void DocumentFrequency(){
		int nrDoc = fileList.size();
		
		for(String word : searchQuery){
			//Add Threads
			List<MapHelper> wordIndexFiles = new ArrayList<MapHelper>();
			int appearance = nrDoc;
			wordIndexFiles = map.get(word);
			
			for(MapHelper mapIndexFile : fileList){
				if(wordIndexFiles.contains(mapIndexFile))
					continue;
				else
					appearance--;
			}
			
			double idf = Math.log((double)nrDoc / appearance);
			idfMap.put(word, idf);
		}
	}
	
	public void SetQueryParams(){
		int wordCount = searchQuery.length;
		this.query = new Double[wordCount];
		int index = 0;
		
		for(String word : searchQuery){
			query[index] = ((double)1 / wordCount) * idfMap.get(word);
			index++;
		}
	}
	
	public void SetDistances(){
		for(Map.Entry<String, Double[]> document : tfMap.entrySet()){
			double vectMultiplySum = 0;
			double moduleDocSum = 0;
			double moduleQuerySum = 0;
			Double[] currentTf = document.getValue();
			
			for(int i=0; i < currentTf.length; i++){
				vectMultiplySum += currentTf[i] * query[i];
				moduleDocSum += currentTf[i] * currentTf[i];
				moduleQuerySum += query[i] * query[i];
			}
			
			double dist = vectMultiplySum / (Math.sqrt(moduleDocSum) * Math.sqrt(moduleQuerySum));
			
			distance.put(document.getKey(), dist);
		}
	}
}

